{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>NLP PIPELINES</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is NLP Pipeline\n",
    "\n",
    "NLP pipeline is a **set of steps** followed to build an end-to-end NLP software.\n",
    "\n",
    "OR\n",
    "\n",
    "An NLP (Natural Language Processing) pipeline is a structured sequence of steps used to develop an end-to-end NLP software application. Each stage of the pipeline contributes to transforming raw text data into actionable insights or predictions through systematic processing.\n",
    "\n",
    "- NLP Software consists of the following steps:\n",
    "    - [1. Data Acquisition](#1-data-acquisition)\n",
    "    - [2. Text Preparation](#2-text-preparation)\n",
    "        - Text Clean-Up: basic steps to remove spelling errors, emojis, etc.\n",
    "        - Basic Preprocessing: tokenization\n",
    "        - Advance Preprocessing: Parts of speech tagging, chucking, co-reference resolution\n",
    "    - [3. Feature Engineering](#3-feature-engineering)\n",
    "        - Bag of words\n",
    "        - TFIDF\n",
    "        - One Hot Encoding\n",
    "        - Word2Vec\n",
    "    - [4. Modelling](#4-modelling)\n",
    "        - Model Building\n",
    "        - Evaluation\n",
    "    - [5. Deployment](#5-deployment)\n",
    "        - Deployment on any cloud service\n",
    "        - Monitoring\n",
    "        - Model Update "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Acquisition\n",
    "\n",
    "**Objective**: \n",
    "\n",
    "Collect and gather relevant data for analysis. This step is crucial as the quality and quantity of data can significantly impact the performance of the NLP model.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "**Top 3 Data Sources**:\n",
    "\n",
    "1. **Internal Data(available internally)**\n",
    "    - **Available on Desktop**: If data is readily available on local systems, it can be directly accessed for analysis.\n",
    "        - Move to next step of the pipelines\n",
    "    - **On Database/Company Systems**: Request data from the data engineering team if it resides in company databases or data warehouses.\n",
    "    - **Data Augmentation**: If data is limited, generate synthetic data using techniques such as:\n",
    "        - Generate fake/synthethic data\n",
    "            - **Synonyms**: Replace words with their synonyms.\n",
    "            - **Bigram Flip**: Swap words in pairs (bigrams) to create variations.\n",
    "            - **Back Translation**: Translate text to another language and then back to the original language to create paraphrased data.\n",
    "            - **Adding Noise**: Introduce random noise to the data to increase robustness.\n",
    "\n",
    "\n",
    "2. **External Data(available externally to someone else)**\n",
    "    - **Public Datasets**: Utilize datasets available on platforms like Kaggle, academic institutions, or data repositories.\n",
    "    - **Web Scraping**: Extract data from websites using tools like BeautifulSoup.\n",
    "    - **Web APIs**: Access data through APIs provided by services such as RapidAPI.\n",
    "    - **PDF/Image/Audio**: Use OCR (Optical Character Recognition) for PDFs, and audio processing techniques for audio data.\n",
    "\n",
    "\n",
    "3. **No Existing Data**\n",
    "    There is no data about the topic, it's first time to began with\n",
    "    - **Customer Reviews**: Collect feedback from loyal customers or stakeholders to gather initial data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Preparation\n",
    "\n",
    "**Objective**: \n",
    "\n",
    "Process and clean text data to make it suitable for feature extraction and modeling.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "The top 3 works to perform in this stage:\n",
    "1. **Basic Clean-Up**:\n",
    "    - **HTML Tag Cleaning**: Remove HTML tags from web-scraped content.\n",
    "    - **Removing Emojis**: Eliminate emojis and other non-standard characters.\n",
    "    - **Checking Spelling**: Correct spelling errors to avoid confusion in further analysis.\n",
    "\n",
    "2. **Basic Text Preprocessing**:\n",
    "    - **Basic / Fundamental**\n",
    "        - **Tokenization**: Split text into tokens.\n",
    "            - Word\n",
    "            - Sentence\n",
    "    - **Optional** \n",
    "        - **Stop Word Removal**: Remove common words that add little value (e.g., \"the,\" \"and\").\n",
    "        - **Stemming/Lemmatization**: Reduce words to their base or root form.\n",
    "        - **Removing Punctuation/Digits**: Eliminate punctuation and numeric values if not relevant.\n",
    "        - **Lowercasing/Uppercasing**: Convert text to a consistent case.\n",
    "        - **Language Detection**: Identify and process text based on language.\n",
    "\n",
    "        \n",
    "3. **Advance Text Preprocessing**:\n",
    "    - **Parts of Speech Tagging**: Assign grammatical categories to each word (e.g., noun, verb).\n",
    "    - **Parsing**: Analyze sentence structure to understand syntactic relationships.\n",
    "    - **Co-reference Resolution**: Identify and link pronouns to their corresponding entities (e.g., \"he\" to \"Avinash\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello my name is Avinash Yadav. Below is the sample code for HTML Tag cleaning using regular expression.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing HTML Tags:\n",
    "\n",
    "# Sample HTML data with various HTML tags\n",
    "html_data = \"<p>Hello my name is <strong>Avinash Yadav</strong>. Below is the sample code for <em>HTML Tag cleaning</em> using regular expression.</p>\"\n",
    "\n",
    "# Import the 're' module for regular expressions\n",
    "import re\n",
    "\n",
    "# Define a function to remove HTML tags from a string\n",
    "def stripHTML(data):\n",
    "    # Compile a regular expression pattern to match HTML tags\n",
    "    # The pattern '<.*?>' matches any text between '<' and '>', including the tags\n",
    "    p = re.compile(r'<.*?>')\n",
    "    \n",
    "    # Substitute (remove) all HTML tags in the input data with an empty string\n",
    "    # This effectively removes all the HTML tags from the string\n",
    "    return p.sub('', data)\n",
    "\n",
    "stripHTML(html_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello \\xf0\\x9f\\x91\\x8b my name is Avinash Yadav \\xf0\\x9f\\x99\\x82. Below \\xf0\\x9f\\x91\\x87 is the sample code for removing emojis \\xf0\\x9f\\xab\\xa2 using unicode normalization. \\xe2\\x86\\x97\\xef\\xb8\\x8f'\n"
     ]
    }
   ],
   "source": [
    "# Unicode Normalization\n",
    "\n",
    "# Sample string containing emojis and various Unicode characters\n",
    "emoji_data = 'Hello üëã my name is Avinash Yadav üôÇ. Below üëá is the sample code for removing emojis ü´¢ using unicode normalization. ‚ÜóÔ∏è'\n",
    "\n",
    "# Encode the string in UTF-8 format\n",
    "encoded_data = emoji_data.encode('utf-8')\n",
    "\n",
    "# Print the encoded data to show its byte representation\n",
    "print(encoded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lo! Low is the sample code for choking swellings. That is or name? Now r u?\n"
     ]
    }
   ],
   "source": [
    "# Sample text with spelling errors\n",
    "incorrect_data = 'Hlo! Blow is the sampl code for chking spellings. What is ur name? How r u?'\n",
    "\n",
    "# Import the TextBlob library for natural language processing tasks\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Create a TextBlob object with the incorrect_data string\n",
    "TextBlb = TextBlob(incorrect_data)\n",
    "\n",
    "# Use the correct() method to correct spelling errors in the text\n",
    "corrected_text = TextBlb.correct()\n",
    "\n",
    "# Print the corrected text\n",
    "print(corrected_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lorem Ipsum.', 'Lorem Ipsum.', 'Lorem Ipsum.', 'Lorem Ipsum.']\n",
      "['Lorem', 'Ipsum', '.', 'Lorem', 'Ipsum', '.', 'Lorem', 'Ipsum', '.', 'Lorem', 'Ipsum', '.']\n"
     ]
    }
   ],
   "source": [
    "# Sample text to be tokenized\n",
    "dummy_data = \"Lorem Ipsum. Lorem Ipsum. Lorem Ipsum. Lorem Ipsum.\"\n",
    "\n",
    "# Import the tokenization functions from the nltk library\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Tokenize the text into sentences\n",
    "sents = sent_tokenize(dummy_data)\n",
    "print(sents)\n",
    "\n",
    "# Tokenize the text into words\n",
    "words = word_tokenize(dummy_data)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "**Objective**: \n",
    "\n",
    "Convert text data into numerical features that can be used for modelling\n",
    "\n",
    "(The step to convert the text into numbers **OR** the process to extract the input column from the text is called as feature engineering.)\n",
    "\n",
    "Features are the input columns in machine learning.\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "**Common Techniques**:\n",
    "- **Bag of Words (BoW)**: Represent text as a collection of word frequencies.\n",
    "- **TF-IDF (Term Frequency-Inverse Document Frequency)**: Weigh words based on their importance in a document relative to a corpus.\n",
    "- **One-Hot Encoding**: Represent words or tokens as binary vectors.\n",
    "- **Word2Vec**: Map words to dense vector representations capturing semantic meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modelling\n",
    "\n",
    "\n",
    "**Objective**: \n",
    "\n",
    "Apply machine learning or deep learning algorithms to the processed data to build and evaluate models.\n",
    "\n",
    "The stage where the algorithms are applied on the processed data and evaluate it.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "There are 2 steps in this stage:\n",
    "1. **Model Building / Modelling**:\n",
    "\n",
    "    The kind of model selection depends on *1. Amount of available data* & *2. Nature of the problem*\n",
    "    - **Heuristic Approaches**: Simple rule-based models or algorithms based on predefined rules.\n",
    "    - **ML Algorithms**: Machine learning techniques like Naive Bayes, SVM, etc.\n",
    "    - **DL Algorithms**: Deep learning methods like neural networks, RNNs, etc.\n",
    "    - **Cloud APIs**: Utilize pre-built models from cloud services (e.g., Google Cloud AI, AWS Comprehend).\n",
    "\n",
    "\n",
    "2. **Evaluation**:\n",
    "    - **Intrinsic Evaluation**: Assess model performance using metrics like accuracy, precision, recall, F1 score.\n",
    "    - **Extrinsic Evaluation**: Evaluate the model's effectiveness based on its impact or performance in real-world applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Deployment\n",
    "\n",
    "**Objective**: \n",
    "\n",
    "Deploy the NLP model into a production environment and ensure its continued performance and relevance.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "The 3 main stages are:\n",
    "1. **Deployment**:\n",
    "    - **Cloud Services**: Deploy the model on platforms like AWS, Google Cloud, or Azure.\n",
    "\n",
    "2. **Monitoring**:\n",
    "    - **Performance Tracking**: Continuously monitor model performance and accuracy.\n",
    "    - **Error Analysis**: Identify and address any issues or drifts in performance.\n",
    "\n",
    "3. **Model Update**:\n",
    "    - **Retraining**: Update and retrain the model periodically with new data.\n",
    "    - **Versioning**: Maintain versions of the model to manage changes and improvements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
